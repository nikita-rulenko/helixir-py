# Helixir

**Persistent Memory Framework for AI Assistants powered by HelixDB**

[![Python 3.14+](https://img.shields.io/badge/python-3.14+-blue.svg)](https://www.python.org/downloads/)
[![License: AGPL-3.0](https://img.shields.io/badge/License-AGPL%20v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)

## ğŸ¯ What is Helixir?

Helixir is a Python framework for building **persistent memory systems** for AI assistants. Unlike simple context windows, Helixir provides:

- ğŸ§  **Ontological Graph**: Hierarchical concept taxonomy with semantic relationships
- ğŸ”— **Reasoning Chains**: IMPLIES, BECAUSE, CONTRADICTS edges for logical inference
- â° **Temporal Awareness**: Search by time window (recent, contextual, deep, full)
- ğŸ¯ **Concept-Based Search**: Find memories by skill, preference, goal, fact
- ğŸ” **Hybrid Search**: Vector + Graph + BM25 in a single query
- âš¡ **Single Database**: HelixDB handles graph, vector, and full-text search

## ğŸš€ Quick Start

See the main [README](../README.md) for installation instructions.

### Basic Usage

```python
from helixir import HelixirClient

# Initialize from config
client = HelixirClient.from_yaml("config.yaml")

# Add a memory
result = await client.add(
    content="User prefers Python over JavaScript",
    memory_type="preference",
    user_id="alice",
    context_tags="programming,languages"
)

# Search memories
results = await client.search(
    query="What programming languages does Alice like?",
    user_id="alice",
    mode="contextual"  # recent, contextual, deep, full
)

# Search by concept type
skills = await client.search_by_concept(
    query="programming",
    user_id="alice",
    concept_type="skill"
)

# Get reasoning chain
chain = await client.search_reasoning_chain(
    query="Why does user prefer Python?",
    user_id="alice",
    chain_mode="causal"  # causal, forward, both
)
```

## ğŸ—ï¸ Architecture

```
helixir/
â”œâ”€â”€ core/               # Client, config, exceptions
â”‚   â”œâ”€â”€ helixir_client.py   # Main entry point
â”‚   â””â”€â”€ config.py           # Configuration management
â”œâ”€â”€ llm/                # LLM providers
â”‚   â”œâ”€â”€ factory.py          # Provider factory (singleton)
â”‚   â”œâ”€â”€ embeddings.py       # Embedding generation
â”‚   â””â”€â”€ providers/          # Cerebras, Ollama, OpenAI
â”œâ”€â”€ toolkit/
â”‚   â””â”€â”€ mind_toolbox/
â”‚       â”œâ”€â”€ memory/         # Memory CRUD
â”‚       â”œâ”€â”€ search/         # Search strategies
â”‚       â”œâ”€â”€ memory_chain/   # Reasoning chains
â”‚       â””â”€â”€ ontology/       # Concept management
â”œâ”€â”€ mcp/                # MCP server for Cursor/Claude
â”‚   â””â”€â”€ server.py
â””â”€â”€ setup/              # Installation wizard
    â”œâ”€â”€ wizard.py
    â”œâ”€â”€ deploy_schema.py
    â””â”€â”€ seed_memories.py
```

## ğŸ”§ Configuration Priority

```
ENV (mcp.json) > YAML (config.yaml) > Defaults
```

Environment variables override YAML, which overrides defaults.

## ğŸ› ï¸ Development

```bash
# Create virtual environment
uv venv --python 3.14
source .venv/bin/activate

# Install dependencies
uv sync

# Run tests
uv run pytest

# Format code
uv run ruff format .

# Lint
uv run ruff check .
```

## ğŸ“„ License

**AGPL-3.0** (with Commercial License option)

See [LICENSE.txt](../LICENSE.txt) for details. This is NOT MIT - if you deploy as SaaS, you must open-source your code or get a commercial license.
